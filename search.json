[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "首页",
    "section": "",
    "text": "test"
  },
  {
    "objectID": "posts/SFT与RL数据配比.html",
    "href": "posts/SFT与RL数据配比.html",
    "title": "Adam Optimizer",
    "section": "",
    "text": "猫猫镇楼\n\n\n\n\n\n日期\n模型\n方法/说明\n\n\n\n\n2025/0122\nDeepSeek-R1\nDeepSeek-R1 (MoE): SFT-RL-SFT-RL  Llama-3.1-8B (Dense): Distillation: 800k SFT\n\n\n2025/0122\nKimi k1.5\nonline mirror descent\n\n\n\n\nSFT有什么用"
  },
  {
    "objectID": "posts/adam.html",
    "href": "posts/adam.html",
    "title": "Adam Optimizer",
    "section": "",
    "text": "猫猫镇楼\n\n\n这篇博文是讲Adam优化器，Adaptive Moment Estimation\n\n基本介绍\n核心想法：Adam是针对每一个参数更新的，用一阶矩和二阶矩更新。\n公式为：\n\\[\\theta_t = \\theta_{t-1} - \\eta \\cdot \\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t}} + \\epsilon}\\]\n\n一阶矩（动量）更新： \\[m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t, \\quad m_0 = 0\\]\n二阶矩（方差）更新： \\[v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2, \\quad v_0 = 0\\]\n偏差校正： \\[\\hat{m_t} = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v_t} = \\frac{v_t}{1-\\beta_2^t}\\]\n\n符号说明：\n\n梯度： \\(g_t\\)\n超参（意味着与核心公式无关）： \\(\\eta, \\epsilon, \\beta_1, \\beta_2\\)\n\n\n\n感性理解\n\n一阶矩控制更新方向，二阶矩控制更新速度\n一阶矩\\(m_t\\)也是一种梯度，是过往梯度的“融合”。\\(\\beta_1\\)一般设做\\(0.9\\)，所以每次计算的新梯度都以10%的作用生效，然后逐渐递减\n\n\n\n\n梯度在更新中的贡献"
  },
  {
    "objectID": "posts/muon_muonclip.html",
    "href": "posts/muon_muonclip.html",
    "title": "Muon Optimizer",
    "section": "",
    "text": "猫猫镇楼\n这篇博文是讲Muon优化器，Adaptive Moment Estimation"
  },
  {
    "objectID": "posts/muon_muonclip.html#newton-method切线找根",
    "href": "posts/muon_muonclip.html#newton-method切线找根",
    "title": "Muon Optimizer",
    "section": "Newton Method：切线找根",
    "text": "Newton Method：切线找根\n\n\n\nNewton_Method"
  },
  {
    "objectID": "posts/muon_muonclip.html#newton-schulz-method牛顿法的矩阵类似",
    "href": "posts/muon_muonclip.html#newton-schulz-method牛顿法的矩阵类似",
    "title": "Muon Optimizer",
    "section": "Newton-Schulz Method：牛顿法的矩阵类似",
    "text": "Newton-Schulz Method：牛顿法的矩阵类似\nMotivation：把原本的矩阵逐渐迭代成正交阵 一次迭代： \\[\n\\begin{aligned}\nG' &:= aG + b(GG^\\top)G + c(GG^\\top)^2 G \\\\\n   &= (aI + b(GG^\\top) + c(GG^\\top)^2)G \\\\\n   &= (aI + bUS^2U^\\top + cUS^4U^\\top)USV^\\top \\\\\n   &= U(aS + bS^3 + cS^5)V^\\top\n\\end{aligned}\n\\]\n其中第二行到第三行应用了SVD分解 原理：中间的奇异值S逐渐趋近于1"
  },
  {
    "objectID": "posts/muon_muonclip.html#muon-optimizer-1",
    "href": "posts/muon_muonclip.html#muon-optimizer-1",
    "title": "Muon Optimizer",
    "section": "Muon Optimizer",
    "text": "Muon Optimizer\n# Pytorch code\ndef newtonschulz5(G, steps=5, eps=1e-7):\n    assert G.ndim == 2\n    a, b, c = (3.4445, -4.7750, 2.0315) ------------------------&gt; 满足条件二\n    X = G.bfloat16()\n    X /= (X.norm() + eps)  -------------------------------------&gt; 满足条件一\n    if G.size(0) &gt; G.size(1):\n        X = X.T\n    for _ in range(steps):\n        A = X @ X.T\n        B = b * A + c * A @ A\n        X = a * X + B @ X\n    if G.size(0) &gt; G.size(1):\n        X = X.T\n    return X\n应用上述Newton-Schulz的充分条件：\n\n初始G的奇异值范围在[0,1]之间，所以每次会除以F范数\n当N趋于1时，(phi(x))^N趋于1，所以需要选择合适的a、b、c\n\n完整算法\n\n第四行：加momentum\n第五行：正交化\n\n\n\n\n\n\n\nAlgorithm Muon\n\n\n\nRequire: Learning rate \\(\\eta\\), momentum \\(\\mu\\)\n\nInitialize \\(B_0 \\gets 0\\)\n\nFor \\(t = 1, \\dots\\) do\n\nCompute \\(G_t \\gets \\nabla_\\theta \\mathcal{L}_t(\\theta_{t-1})\\)\n\n\\(B_t \\gets \\mu B_{t-1} + G_t\\)\n\n\\(O_t \\gets \\text{NewtonSchulz5}(B_t)\\)\n\nUpdate \\(\\theta_t \\gets \\theta_{t-1} - \\eta O_t\\)\n\n\nreturn \\(\\theta_t\\)"
  },
  {
    "objectID": "posts/DeepSeek-R1.html",
    "href": "posts/DeepSeek-R1.html",
    "title": "Adam Optimizer",
    "section": "",
    "text": "猫猫镇楼\n\n\n\n结论\n这一章节Deepseek-R1的一些特别结论, 结论比较分散\n\n代码数学训练集应该先训练\n实验证明，先做代码训练再做数学训练，比从通用 LLM 开始更有利于推理能力\n\n\n统一范式\n他们把 SFT、RFT、DPO、PPO、GRPO 放到一个统一框架下看待，强调这些方法本质上都是“简化版的 RL”\n\n\n强化学习 (RL) 在做什么\nRL 的作用更多是让输出分布更稳定，把正确答案更容易排在 Top-K，而不是提升“基本能力”\n\n\n\nFollow的一些工作\nTinyZero\nsimpleRL-reason: longCoT可以出现在7B模型上"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "首页",
    "section": "",
    "text": "这里以后会写一些科研随笔、笔记和文章。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdam Optimizer\n\n\n\n\n\n\nSFT\n\n\nRL\n\n\n\n\n\n\n\n\n\nSep 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAdam Optimizer\n\n\n\n\n\n\nRL\n\n\nOptimizer\n\n\n\n\n\n\n\n\n\nAug 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMuon Optimizer\n\n\n\n\n\n\nRL\n\n\nOptimizer\n\n\n\n\n\n\n\n\n\nAug 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAdam Optimizer\n\n\n\n\n\n\nRL\n\n\nOptimizer\n\n\n\n\n\n\n\n\n\nAug 27, 2025\n\n\n\n\n\n\nNo matching items"
  }
]