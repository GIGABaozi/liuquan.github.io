---
title: "Adam Optimizer"
date: 2025-08-27
categories: [RL, Optimizer]
---

![猫猫镇楼](../images/test.png)

这篇博文是讲Adam优化器，Adaptive Moment Estimation

# 基本介绍

**核心想法**：Adam是针对每一个参数更新的，用一阶矩和二阶矩更新。

公式为：

$$\theta_t = \theta_{t-1} - \eta \cdot \frac{\hat{m_t}}{\sqrt{\hat{v_t}} + \epsilon}$$

- 一阶矩（动量）更新：
$$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$$

- 二阶矩（方差）更新：
$$v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$$

- 偏差校正：
$$\hat{m_t} = \frac{m_t}{1-\beta_1^t}, \quad \hat{v_t} = \frac{v_t}{1-\beta_2^t}$$

符号说明：

- 梯度：
$g_t$

- 超参（意味着与核心公式无关）：
$\eta, \epsilon, \beta_1, \beta_2$


# 感性理解




