---
title: "Adam Optimizer"
date: 2025-08-27
categories: [RL, Optimizer]
---

![猫猫镇楼](../images/maomao1.png)

这篇博文是讲Adam优化器，Adaptive Moment Estimation

# 基本介绍

**核心想法**：Adam是针对每一个参数更新的，用一阶矩和二阶矩更新。

公式为：

$$\theta_t = \theta_{t-1} - \eta \cdot \frac{\hat{m_t}}{\sqrt{\hat{v_t}} + \epsilon}$$

- 一阶矩（动量）更新：
$$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t, \quad m_0 = 0$$

- 二阶矩（方差）更新：
$$v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2, \quad v_0 = 0$$

- 偏差校正：
$$\hat{m_t} = \frac{m_t}{1-\beta_1^t}, \quad \hat{v_t} = \frac{v_t}{1-\beta_2^t}$$

符号说明：

- 梯度：
$g_t$

- 超参（意味着与核心公式无关）：
$\eta, \epsilon, \beta_1, \beta_2$


# 感性理解

- 一阶矩控制更新方向，二阶矩控制更新速度
- 一阶矩$m_t$也是一种梯度，是过往梯度的“融合”。$\beta_1$一般设做$0.9$，所以每次计算的新梯度都以10%的作用生效，然后逐渐递减

![梯度在更新中的贡献](../images/adam_gradient_explain.png)



